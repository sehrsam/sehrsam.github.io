[
  {
    "objectID": "posts/Timnit_Gebru/Timnit_Gebru_Blog_post.html",
    "href": "posts/Timnit_Gebru/Timnit_Gebru_Blog_post.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This week, Timnit Gebru is going to be talking to our Machine Learning class, and will be giving a talk that is open for anyone to attend on Thursday at 7 pm. Gebru is a stanford educated computer science researcher who has worked for companies such as Google, Amazon and Microsoft, though she currently works as an independent researcher. Her research has been focused on the effects of algorithmic bias and how machine learning models often discriminate against people, such as in her paper “Gender Shades” which exposed how facial recognition software was significantly worse at distinguishing faces of women and people of color, or in the paper that led to her firing at Google, “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”.\nIn her talk, Gebru discusses the issues of machine learning processes on real world data. One thing that she mentions is how computer scientists often speak and act as if what they do somehow transcends the world around them, as if they are not affected by the realities of the world, and so their algorithms and research should similarly be above this. She brings up a quote from Seeta Pena Gangadharan, which highlights how computer scientists think of their work in terms of how it can be used to examine the real world, ignoring how the real world impacts the decisions and algorithms that are made.\nGebru then discusses her study Gender Shades, discussing how she and her research partner went about it and some of the flawed reasoning behind the study. One such reason is the issue with race, where she highlighted how race is a social construct, and how a person’s “race” is dependent upon who is observing, giving examples such as how during apartheid era South Africa a chinese person would be classified as black while a japanese one would be classified as white. She then discusses how the publicly available datasets where mainly white and mainly male, before showing how datasets in general are extremely euro-centric, meaning that computer vision processes will be less useful in many areas.\nShe then shows examples of of how bias inherent in a dataset can be amplified by machine learning algorithms, using images of cooking, soap vs spices and weddings, where algorithms misclassified images because they did not fit with eurocentric norms. She then discusses how these issues apply to not only machine learning models but also in regular studies, such as in crash test dummies or drug trials, the results of which led to women and children being disproportionately harmed.\nTimnit then discusses how people have begun to acknowledge that these datasets may be flawed, but that little has been done to fix them, and that which has been done has been done in a predatory manner. She then mentions how even when diverse datasets are made, they often are used in ways that harm people, and seldom are the peoples whose faces and data are used see any benefits. She shows how police have used facial recognition software in a completely unregulated manner, and how even when there are systems in place they often ignore them, and diminish civil liberties. She then mentions how these tools are often trusted because of the way that people view automation, which can lead to people being unjustly incarcerated.\nShe then talks about who is affected by computer vision the most, showing how it is minorities and marginalized communities, and how even when there are large groups protesting and acting against these issues they are often made up of people who have power and influence, thus not the marginalized minorities.\ntl/dr: In this talk, Timnit Gebru discusses the historical, present and future issues surrounding computer vision and facial recognition databases.\nQuestion: Do you think that the ethical concerns around computer vision are great enough that you think no work should be done in the field, as any progress even towards a noble goal could be used to diminish civil liberties and harm people?"
  },
  {
    "objectID": "posts/Penguins/Penguins.html",
    "href": "posts/Penguins/Penguins.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      270\n      PAL0708\n      7\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N34A1\n      Yes\n      11/27/07\n      45.4\n      14.6\n      211.0\n      4800.0\n      FEMALE\n      8.24515\n      -25.46782\n      NaN\n    \n    \n      271\n      PAL0910\n      121\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N66A1\n      No\n      11/17/09\n      36.2\n      17.2\n      187.0\n      3150.0\n      FEMALE\n      9.04296\n      -26.19444\n      Nest never observed with full clutch.\n    \n    \n      272\n      PAL0910\n      84\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N6A2\n      Yes\n      11/15/09\n      50.0\n      15.9\n      224.0\n      5350.0\n      MALE\n      8.20042\n      -26.39677\n      NaN\n    \n    \n      273\n      PAL0708\n      29\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N47A1\n      Yes\n      11/29/07\n      48.2\n      14.3\n      210.0\n      4600.0\n      FEMALE\n      7.68870\n      -25.50811\n      NaN\n    \n    \n      274\n      PAL0708\n      13\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N7A1\n      Yes\n      11/15/07\n      41.1\n      17.6\n      182.0\n      3200.0\n      FEMALE\n      NaN\n      NaN\n      Not enough blood for isotopes.\n    \n  \n\n275 rows × 17 columns\n\n\n\n\nt=train.groupby([\"Species\",\"Sex\"]).agg({\"Body Mass (g)\":[\"mean\",\"std\"],\"Flipper Length (mm)\":[\"mean\",\"std\"], \"Culmen Depth (mm)\":[\"mean\",\"std\"],\"Culmen Length (mm)\":[\"mean\",\"std\"] })\n\nt\n\n\n\n\n\n  \n    \n      \n      \n      Body Mass (g)\n      Flipper Length (mm)\n      Culmen Depth (mm)\n      Culmen Length (mm)\n    \n    \n      \n      \n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n    \n    \n      Species\n      Sex\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      3337.280702\n      267.558522\n      187.719298\n      5.966437\n      17.645614\n      0.900609\n      37.100000\n      2.108825\n    \n    \n      MALE\n      4020.454545\n      332.467309\n      192.690909\n      6.440246\n      19.116364\n      1.076304\n      40.458182\n      2.228161\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      3514.655172\n      295.843018\n      191.551724\n      5.295458\n      17.641379\n      0.801569\n      46.424138\n      2.390704\n    \n    \n      MALE\n      3936.111111\n      396.216077\n      199.666667\n      6.244998\n      19.303704\n      0.801672\n      51.185185\n      1.689266\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      .\n      4875.000000\n      NaN\n      217.000000\n      NaN\n      15.700000\n      NaN\n      44.500000\n      NaN\n    \n    \n      FEMALE\n      4677.976190\n      283.015253\n      212.928571\n      4.050853\n      14.242857\n      0.539712\n      45.600000\n      2.117177\n    \n    \n      MALE\n      5502.314815\n      302.671615\n      221.462963\n      5.578642\n      15.687037\n      0.719551\n      49.592593\n      2.713478\n    \n  \n\n\n\n\nThis table shows the variation between male and female members of each species. It shows the mean of a given value, as well as the standard deviation, allowing us to see which of these features are the most or least varied amongst a population. From this table we can see that the males of each species are larger than the females in all of our categories. We can also see that the Gentoo penguins are the most uniform in size, as the standard deviations for the features are smaller than those of the other species, despite them being the largest in most categories.\n\nimport seaborn as sns\nsns.set_theme(style=\"white\")\n\n\nsns.relplot(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", hue=\"Species\", style = (\"Island\"),\n            sizes=(40, 400), alpha=.5, palette=\"muted\",\n            height=6, data=train)\n\n<seaborn.axisgrid.FacetGrid at 0x7fb1d5f8ce80>\n\n\n\n\n\nFrom this figure we can see several things. First, we can see that while Adelie and Chinstrap Penguins are pretty in terms of flipper length and body mass, Gentoo penguins are significantly larger. We can also see that Adelie penguins are the only ones that are on multiple islands. We can also see that there is a positive linear correlation between body mass and flipper length.\n\nX_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom itertools import combinations\n\nFRC = RandomForestClassifier()\n\n\n\n\n\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)','Delta 15 N (o/oo)','Delta 13 C (o/oo)']\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\n\nscore_1_cols = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n    FRC.fit(X_train[cols],y_train)\n    if (FRC.score(X_train[cols],y_train)== 1.0):\n        score_1_cols.append(cols)\nprint(score_1_cols[17])\n\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Delta 15 N (o/oo)']\n\n\n\nFRC.fit(X_train[score_1_cols[17]],y_train)\nprint (len(score_1_cols))\nFRC.score(X_train[score_1_cols[17]],y_train)\n\n36\n\n\n1.0\n\n\nUsing the Random Forest Classifier on all possible combinations of qualitative and quantitative data we get 36 combinations that achieve a training score of 1.0. I will select the 17th column to look at.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nFRC.score(X_test[score_1_cols[17]], y_test)\n\n0.8235294117647058\n\n\nWe can see this column has a validation score of 0.8235, a very good score. If we plot it to get the decision boundaries we get:\n\nfrom matplotlib.patches import Patch\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(FRC, X_train[score_1_cols[17]], y_train)"
  },
  {
    "objectID": "posts/perceptron/Perceptron_blog_post.html",
    "href": "posts/perceptron/Perceptron_blog_post.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "The code for my update function is as follows: self.w += (2*(int(self.w@X_[var_check].T <0))-1)*X_[var_check]\nself.w is the weight function which is stored as an instance variable of the perceptron. The 2*(int(self.w@X_[var_check].T)) will determine if we are adding or subtracting to w, creating a value of -1 or 1. *X_[var_check].T tells it the magnitude to increase or decrease w by. var_check is our randomly selected variable that we use to check and adjust w.\nIn this experiment, we will use two linearly seperable groups of data points, and run our perceptron on them.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(57698)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nHere we have plotted the points. We can clearly see that they are linearly seperable. We now run our perceptron algorithm on them to find it.\n\nfrom perceptron import Perceptron\np = Perceptron()\np.fit(X, y, 1000)\nprint(p.history)\n\n[0.02, 0.02, 0.48, 0.99, 0.13, 0.56, 0.05, 0.92, 0.03, 0.9, 0.0, 0.63, 0.0, 0.03, 1.0]\n\n\nAfter running the algorithm, we can print the history variable of the perceptron. This allows us to view the progression of our perceptron, and see how it oscillates between being more and less accurate before arriving at a w value which gives us a linear classifier.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nWe can then graph this linear classifier, and see how it neatly bisects our two groups of data points.\nNext, we will examine two groups of data points which are not linearly seperable.\n\n\nn = 100\np_features = 3\n\nX_1, y_1 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0,0), (0,0)])\n\nfig = plt.scatter(X_1[:,0], X_1[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nAfter graphing the points, it is clear that they are not linearly seperable. As such, if we run our perceptron on them we would expect it to not find a linear classifier, and to run until it has reached the maximum allowed runs\n\n\np.fit(X_1, y_1, 100)\nprint(p.history)\n\nDoing this, we see what we expected. Our perceptron runs for the maximum allowed time, and never comes up with a w that produces a 1.0 accuracy.\n\nfig = plt.scatter(X_1[:,0], X_1[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nPlotting the line given by w we get a random line, which makes sense.\nWe can now try running the perceptron on data with more than 2 features. We can generate the points, and run our perceptron on them.\n\n\nX_2, y_2 = make_blobs(n_samples = 100, n_features = 5, centers=[(1,1,1,0,0),(0,0,0,0,0)])\n\np.fit(X_2, y_2, 200)\n\nprint(p.history)\n\n[0.63, 0.61, 0.64, 0.37, 0.49, 0.61, 0.63, 0.7, 0.57, 0.7, 0.57, 0.58, 0.55, 0.54, 0.51, 0.49, 0.54, 0.71, 0.6, 0.46, 0.64, 0.39, 0.35, 0.31, 0.44, 0.22, 0.4, 0.25, 0.31, 0.54, 0.65, 0.49, 0.56, 0.62, 0.63, 0.36, 0.36, 0.42, 0.34, 0.42, 0.39, 0.59, 0.6, 0.73, 0.51, 0.51, 0.46, 0.51, 0.72, 0.72, 0.78, 0.76, 0.78, 0.62, 0.54, 0.71, 0.56, 0.65, 0.58, 0.49, 0.46, 0.4, 0.49, 0.49, 0.37, 0.37, 0.32, 0.46, 0.56, 0.6, 0.66, 0.28, 0.26, 0.57, 0.36, 0.38, 0.35, 0.26, 0.4, 0.56, 0.5, 0.38, 0.31, 0.26, 0.32, 0.49, 0.5, 0.56, 0.58, 0.59, 0.43, 0.59, 0.59, 0.61, 0.68, 0.43, 0.64, 0.35, 0.42, 0.59, 0.71, 0.6, 0.54, 0.38, 0.39, 0.44, 0.52, 0.53, 0.46, 0.53, 0.41, 0.36, 0.33, 0.31, 0.49, 0.28, 0.27, 0.55, 0.6, 0.36, 0.33, 0.33, 0.52, 0.3, 0.27, 0.22, 0.56, 0.51, 0.33, 0.49, 0.52, 0.54, 0.59, 0.59, 0.38, 0.43, 0.26, 0.48, 0.31, 0.42, 0.58, 0.59, 0.48, 0.65, 0.7, 0.63, 0.59, 0.43, 0.44, 0.36, 0.41, 0.41, 0.45, 0.39, 0.4, 0.53, 0.37, 0.31, 0.3, 0.52, 0.46, 0.37, 0.66, 0.54, 0.68, 0.44, 0.42, 0.73, 0.35, 0.38, 0.42, 0.51, 0.59, 0.43, 0.28, 0.27, 0.25, 0.23, 0.53, 0.23, 0.55, 0.63, 0.54, 0.54, 0.63, 0.62, 0.61, 0.57, 0.41, 0.58, 0.36, 0.3, 0.44, 0.47, 0.56, 0.45, 0.34, 0.6, 0.39, 0.34]\n\n\nThis set of points most likely is not linearly seperable, as the perceptron did not end up arriving at an accuracy of 1.0. If we test again with slightly altered parameters:\n\nX_2, y_2 = make_blobs(n_samples = 100, n_features = 7, centers=[(2,2,2,2,2,2,2),(-1,-1,-1,-1,-1,-1,-1)])\n\np.fit(X_2, y_2, 200)\nprint(p.history)\n\nWe can see that the perceptron can still converge on data sets with multiple functions. As such, we can also determine that this set of points are linearly seperable.\nThe runtime of equation 1 is O(p), since the dot product will compute p total multiplications, and it is then added to and multiplied by these same p dimensional arrays, but only a set amount of times, so they don’t add to the complexity."
  },
  {
    "objectID": "posts/Least-Squares/LeastSquaresBlogPost.html",
    "href": "posts/Least-Squares/LeastSquaresBlogPost.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import numpy as np\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(37638)\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nThe first thing that we want to do is to create our set of test datapoints. When p_features = 1, we can visualize them like so.\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train)\n\n\nIf we use the analytic method of fit, we get the following scores:\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.6848\nValidation score = 0.7319\n\n\nSince a score is better the higher that it is, noting that the score cannot exceed 1, the scores of 0.648 and 0.739 are solid. However, the analytic method is not the only way to implement least squares linear regression. We are also able to use gradient descent to set our weights. Using it, we get the following scores:\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8786\nValidation score = 0.9089\n\n\nThese scores are also very good. We can go further, and look at how this score changes over time by taking a look at the score history, and see how it improves over iterations.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nWe can experiment with using more features, seeing how it changes the effectiveness of our models. Since there are going to be more than 2 features, it is tough to visualize our data. However, we can still see the effectiveness of our models by using the score function of our linearSquares class.\n\nn_train = 100\nn_val = 100\np_features = 10\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\nWhen we try our analytic method, we get the following scores:\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train) \n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.6702\nValidation score = 0.6666\n\n\nDoing the same with using gradient descent we get:\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\n\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8851\nValidation score = 0.9168\n\n\nThe scores for these are pretty similar to their p_features = 1 counterparts. If we increase the number of features up to 50 we get the following scores for the analytic method:\n\n\n\n\n\n\n\nn_train = 100\nn_val = 100\np_features = 50\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train) \n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9911\nValidation score = 0.9712\n\n\nAnd for gradient descent we get\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.0001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8479\nValidation score = 0.8598\n\n\nThese are still similar. When we increase it to 99 features we get the following scores for the analytic method\n\nn_train = 100\nn_val = 100\np_features = 99\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train) \n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8992\nValidation score = 0.9416\n\n\n\nAnd the following for gradient descent:\n\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = -5.133801039647608e+122\nValidation score = -6.632516808757505e+122\n\n\nFrom this we can see how overfitting affects the gradient descent method.\nWe can also look at the lasso algorithm from sklearn\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\nDoing the same experiments we just did we can see how it does with many features. First with one feature:\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.0803\nValidation score = 0.2088\n\n\nThen with 10:\n\nn_train = 100\nn_val = 100\np_features = 10\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9405\nValidation score = 0.9056\n\n\nThen with 50:\n\nn_train = 100\nn_val = 100\np_features = 50\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9947\nValidation score = 0.9626\n\n\nAnd finally with 99\n\nn_train = 100\nn_val = 100\np_features = 99\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9988\nValidation score = 0.7424\n\n\nWe can see that the validation score remains high even when the number of features is high, which is different to that of the gradient descent"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my blog"
  }
]