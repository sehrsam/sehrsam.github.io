[
  {
    "objectID": "posts/Timnit_Gebru/Timnit_Gebru_Blog_post.html",
    "href": "posts/Timnit_Gebru/Timnit_Gebru_Blog_post.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "This week, Timnit Gebru is going to be talking to our Machine Learning class, and will be giving a talk that is open for anyone to attend on Thursday at 7 pm. Gebru is a stanford educated computer science researcher who has worked for companies such as Google, Amazon and Microsoft, though she currently works as an independent researcher. Her research has been focused on the effects of algorithmic bias and how machine learning models often discriminate against people, such as in her paper “Gender Shades” which exposed how facial recognition software was significantly worse at distinguishing faces of women and people of color, or in the paper that led to her firing at Google, “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”.\nIn her talk, Gebru discusses the issues of machine learning processes on real world data. One thing that she mentions is how computer scientists often speak and act as if what they do somehow transcends the world around them, as if they are not affected by the realities of the world, and so their algorithms and research should similarly be above this. She brings up a quote from Seeta Pena Gangadharan, which highlights how computer scientists think of their work in terms of how it can be used to examine the real world, ignoring how the real world impacts the decisions and algorithms that are made.\nGebru then discusses her study Gender Shades, discussing how she and her research partner went about it and some of the flawed reasoning behind the study. One such reason is the issue with race, where she highlighted how race is a social construct, and how a person’s “race” is dependent upon who is observing, giving examples such as how during apartheid era South Africa a chinese person would be classified as black while a japanese one would be classified as white. She then discusses how the publicly available datasets where mainly white and mainly male, before showing how datasets in general are extremely euro-centric, meaning that computer vision processes will be less useful in many areas.\nShe then shows examples of of how bias inherent in a dataset can be amplified by machine learning algorithms, using images of cooking, soap vs spices and weddings, where algorithms misclassified images because they did not fit with eurocentric norms. She then discusses how these issues apply to not only machine learning models but also in regular studies, such as in crash test dummies or drug trials, the results of which led to women and children being disproportionately harmed.\nTimnit then discusses how people have begun to acknowledge that these datasets may be flawed, but that little has been done to fix them, and that which has been done has been done in a predatory manner. She then mentions how even when diverse datasets are made, they often are used in ways that harm people, and seldom are the peoples whose faces and data are used see any benefits. She shows how police have used facial recognition software in a completely unregulated manner, and how even when there are systems in place they often ignore them, and diminish civil liberties. She then mentions how these tools are often trusted because of the way that people view automation, which can lead to people being unjustly incarcerated.\nShe then talks about who is affected by computer vision the most, showing how it is minorities and marginalized communities, and how even when there are large groups protesting and acting against these issues they are often made up of people who have power and influence, thus not the marginalized minorities.\ntl/dr: In this talk, Timnit Gebru discusses the historical, present and future issues surrounding computer vision and facial recognition databases.\nQuestion: Do you think that the ethical concerns around computer vision are great enough that you think no work should be done in the field, as any progress even towards a noble goal could be used to diminish civil liberties and harm people?\nIn her talks, both to our class and in her open talk, Timnit Gebru discussed the issues surrounding AI. The main focus however, was on who are the ones who are in control and becoming more influential and powerful as a result. Timnit showed the progression of the eugenics movement, showing how it has often been widely accepted by powerful people, and showed the line from modern eugenics movements such as effective altruism go back to the early eugenics movements with the goal of removing negative traits from the human race. She then showed how various influential tech developers and investors believe these principles.\nShe also talked about how the goal of AGI has influenced the development of AI systems, and how all progress towards this goal has been harmful for many vulnerable peoples. The idea of AGI is also one that leads towards centralization of power, as the ultimate goal of one all powerful machine would mean people would go to it for everything, giving massive amounts of power and influence to whoever has created or designed it, people who Timnit had already shown have very flawed views of where humanity must go in the future.\nAnother thing Timnit discussed in her talks was the flaws in how the media view AI, discussing how the lack of knowledge on how it works and the promises from big companies that they are working on certain things will make investors unwilling to fund small organizations that are actually doing them, even when the large corporations end up making a poor quality product if they even deliver one at all.\nI think that I agree with Timnit on all her points. I think that the thing that people should take away from this is how vile the goals of some of these extremely influential tech people are, and just how much they are able to influence. I think that people also need to realize that for many of them, the creation of a powerful new AI system will not benefit them much, and that it will in reality only serve to further enrich the already wealthy.\nLearning from Timnit was very interesting. I found all of the things she brought up to be fascinating, but they left me rather disillusioned with what I have decided to do for a living. As someone who has always had doubts about what tech companies do to actually help people, having a lot of those doubts validated was rather discouraging for me. After her talks, I am very curious about how I can use my skills to help improve the world, and how I can make sure that a future employer will share these values"
  },
  {
    "objectID": "posts/Penguins/Penguins.html",
    "href": "posts/Penguins/Penguins.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\nnp.random.seed(12345)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nt=train.groupby([\"Species\",\"Sex\"]).agg({\"Body Mass (g)\":[\"mean\",\"std\"],\"Flipper Length (mm)\":[\"mean\",\"std\"], \"Culmen Depth (mm)\":[\"mean\",\"std\"],\"Culmen Length (mm)\":[\"mean\",\"std\"] })\n\nt\n\n\n\n\n\n  \n    \n      \n      \n      Body Mass (g)\n      Flipper Length (mm)\n      Culmen Depth (mm)\n      Culmen Length (mm)\n    \n    \n      \n      \n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n    \n    \n      Species\n      Sex\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      FEMALE\n      3337.280702\n      267.558522\n      187.719298\n      5.966437\n      17.645614\n      0.900609\n      37.100000\n      2.108825\n    \n    \n      MALE\n      4020.454545\n      332.467309\n      192.690909\n      6.440246\n      19.116364\n      1.076304\n      40.458182\n      2.228161\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      FEMALE\n      3514.655172\n      295.843018\n      191.551724\n      5.295458\n      17.641379\n      0.801569\n      46.424138\n      2.390704\n    \n    \n      MALE\n      3936.111111\n      396.216077\n      199.666667\n      6.244998\n      19.303704\n      0.801672\n      51.185185\n      1.689266\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      .\n      4875.000000\n      NaN\n      217.000000\n      NaN\n      15.700000\n      NaN\n      44.500000\n      NaN\n    \n    \n      FEMALE\n      4677.976190\n      283.015253\n      212.928571\n      4.050853\n      14.242857\n      0.539712\n      45.600000\n      2.117177\n    \n    \n      MALE\n      5502.314815\n      302.671615\n      221.462963\n      5.578642\n      15.687037\n      0.719551\n      49.592593\n      2.713478\n    \n  \n\n\n\n\nThis table shows the variation between male and female members of each species. It shows the mean of a given value, as well as the standard deviation, allowing us to see which of these features are the most or least varied amongst a population. From this table we can see that the males of each species are larger than the females in all of our categories. We can also see that the Gentoo penguins are the most uniform in size, as the standard deviations for the features are smaller than those of the other species, despite them being the largest in most categories.\n\nimport seaborn as sns\nsns.set_theme(style=\"white\")\n\n\nsns.relplot(x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", hue=\"Species\", style = (\"Island\"),\n            sizes=(40, 400), alpha=.5, palette=\"muted\",\n            height=6, data=train)\n\n<seaborn.axisgrid.FacetGrid at 0x7fcdf57a5e50>\n\n\n\n\n\nFrom this figure we can see several things. First, we can see that while Adelie and Chinstrap Penguins are pretty in terms of flipper length and body mass, Gentoo penguins are significantly larger. We can also see that Adelie penguins are the only ones that are on multiple islands. We can also see that there is a positive linear correlation between body mass and flipper length.\n\nX_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom itertools import combinations\n\nFRC = RandomForestClassifier()\n\n\n\n\n\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)','Delta 15 N (o/oo)','Delta 13 C (o/oo)']\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\n\nscore_1_cols = []\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n    FRC.fit(X_train[cols],y_train)\n    if (FRC.score(X_train[cols],y_train)== 1.0):\n        score_1_cols.append(cols)\nprint(score_1_cols[17])\n\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Delta 15 N (o/oo)']\n\n\n\ncols = ['Culmen Depth (mm)', 'Delta 15 N (o/oo)','Sex_FEMALE', 'Sex_MALE']\n\nFRC.fit(X_train[cols],y_train)\nprint (len(score_1_cols))\n\nFRC.score(X_train[cols],y_train)\n\n37\n\n\n1.0\n\n\nUsing the Random Forest Classifier on all possible combinations of qualitative and quantitative data we get 36 combinations that achieve a training score of 1.0. I will select the 17th column to look at.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nX_test, y_test = prepare_data(test)\nFRC.score(X_test[cols], y_test)\n\n0.8088235294117647\n\n\nWe can see this column has a validation score of 0.8382, a very good score. If we plot it to get the decision boundaries we get:\n\nfrom matplotlib.patches import Patch\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      plt.tight_layout()\n\n\nFRC.fit(X_train[cols],y_train)\nplot_regions(FRC, X_test[cols], y_test)"
  },
  {
    "objectID": "posts/ML_final/LinearRegressionRFE.html",
    "href": "posts/ML_final/LinearRegressionRFE.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Source code\nhttps://github.com/CeceZiegler1/ML_Final_Proj/blob/main/LinearRegressionAnalytic.py\nBelow, we fit our model on the x_train and and y_train datasets, and then print out the training and validation scores. This model is fitted on all 60 features in the dataset. We can see from the scores, that it is not performing great, as a validation score below 50% indicates we could do better by just randomly selecting. We are going to perform a recursive feature elimination that we also implemented in our source code to see if we can find the optimal number of features to use to obtain the best score.\n\nfrom LinearRegressionAnalytic import LinearRegressionAnalytic\nfrom LinearRegressionAnalytic import rfe\n\n#Seeing how the model performs without RFE\n\nLR = LinearRegressionAnalytic()\nLR.fit(x_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(x_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(x_test, y_test).round(4)}\")\n\nTraining score = 0.5258\nValidation score = 0.4605\n\n\nBelow, we create an array to store the score that is produced with each different number of features used in the model as selected by our RFE. We use a for loop to loop through each value from 1-60 and display the score at each iteration in a graph. What we see from the graph, is even with using fewer features, our score never gets above around 45%. The best scores come around 12-15 features and 55-60 feautres. Even still, the scores at these points aren’t very good. Although we were hoping linear regression would perform well on our dataset, it doesn’t appear to be the case. Because of this, we are going to implement a random forest tree to see if we can obtain a better validation score on our dataset.\n\n\n# compute the score for each value of k\nval_scores = []\ntrain_scores = []\nfor k in range(60):\n    selected_features = rfe(x_train, y_train, k)\n    feature_mask = np.zeros(x_train.shape[1], dtype=bool)\n    #masking to include only the selected features\n    feature_mask[selected_features] = True\n    #subseting x train and test to include only selected feautres\n    X_train_selected = x_train.loc[:, feature_mask]\n    X_test_selected = x_test.loc[:, feature_mask]\n    lr = LinearRegressionAnalytic()\n    #fitting model on selected features\n    lr.fit(X_train_selected, y_train)\n    #appending score to score list\n    val_scores.append(lr.score(X_test_selected, y_test))\n    train_scores.append(lr.score(X_train_selected, y_train))\n\n# plot the results\nimport matplotlib.pyplot as plt\nplt.plot(range(1, 61), val_scores, label='Testing accuracy')\nplt.plot(range(1, 61), train_scores, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\n\n[33, 34, 3, 35, 37, 6, 39, 2, 43, 45, 14, 19, 23, 28]\n\n\nBelow, we will show the 13 most important features as obtained through our rfe.\n\ndata.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\n\n\n\n\n  \n    \n      \n      max_rear_shoulder_x\n      max_rear_shoulder_y\n      range_lead_hip_z\n      max_rear_shoulder_z\n      max_torso_y\n      range_lead_shoulder_z\n      max_torso_pelvis_x\n      min_rfx\n      min_rfy\n      range_rear_shoulder_y\n      range_torso_pelvis_x\n      max_lead_hip_z\n      max_pelvis_y\n    \n  \n  \n    \n      0\n      550.0243\n      514.1198\n      778.1339\n      1188.6807\n      182.7302\n      867.6362\n      182.4743\n      -232.2776\n      -88.0097\n      689.2249\n      233.0842\n      384.3450\n      88.3858\n    \n    \n      1\n      638.6019\n      535.2822\n      960.4793\n      1278.5380\n      196.9712\n      1054.5098\n      236.0902\n      -189.7241\n      -106.2254\n      812.9988\n      306.7874\n      520.8627\n      106.4238\n    \n    \n      2\n      580.0406\n      472.9189\n      784.0413\n      1588.7207\n      248.4432\n      988.9415\n      222.8233\n      -124.4299\n      -84.5785\n      708.1030\n      313.2967\n      433.6955\n      82.5397\n    \n    \n      3\n      635.8561\n      484.2663\n      1036.2757\n      888.1270\n      166.9048\n      1472.7250\n      168.7606\n      -175.8547\n      -122.1629\n      732.5588\n      228.6738\n      489.4716\n      81.4764\n    \n    \n      4\n      566.9714\n      502.2202\n      1093.3019\n      1487.6143\n      191.2448\n      1130.6572\n      220.7400\n      -219.5387\n      -72.5831\n      699.1772\n      286.4758\n      597.7220\n      75.9968\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      631.5529\n      488.3580\n      980.3030\n      1575.2948\n      165.8830\n      1150.6032\n      147.9856\n      -114.1301\n      -173.2356\n      804.6660\n      239.9022\n      354.5130\n      124.7927\n    \n    \n      633\n      571.2316\n      477.7701\n      748.3298\n      1604.9299\n      145.5400\n      1026.0944\n      188.9410\n      -113.4915\n      -157.5923\n      735.1128\n      276.8293\n      324.9995\n      137.1521\n    \n    \n      634\n      549.3600\n      407.3251\n      526.3367\n      1393.4961\n      128.0184\n      1029.3547\n      257.2261\n      -112.7565\n      -111.9854\n      584.3304\n      348.2130\n      207.2101\n      128.8111\n    \n    \n      635\n      623.2650\n      463.8467\n      1248.0062\n      1715.0544\n      136.8013\n      892.8699\n      177.4202\n      -122.3425\n      -161.2802\n      725.1355\n      266.6244\n      282.0038\n      157.1024\n    \n    \n      636\n      599.2501\n      505.9937\n      1433.3273\n      1480.4099\n      143.7898\n      1233.8176\n      169.0549\n      -165.5618\n      -132.0637\n      700.1916\n      234.5590\n      500.9032\n      107.8579\n    \n  \n\n637 rows × 13 columns\n\n\n\n\nx_train = x_train.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\nx_test = x_test.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\nlr.fit(x_train, y_train)\nlr.score(x_test, y_test)\n\n0.386607442390802"
  },
  {
    "objectID": "posts/ML_final/RFE.html",
    "href": "posts/ML_final/RFE.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n#Set up\n\n\nmodel = LinearRegression()\nfeat_select = RFE(model, n_features_to_select = 10, step = 1)\n\nWhen using Recursive Feature Elimination, we need to select a few variables. First, we need to select what model we want to use. Since we are using multiple linear regression, we select linear regression. We also need to select how many features we want to use, and how many we want to remove during each recursion. These we chose arbitrarily for this example to be 10 and 1 respectively.\nIf we then run our RFE, we find that it classified our features as True or False, where True refers to features it selected and False ones it did not.\n\nfeat_select.fit(X,y)\nfeat_select.support_\n\narray([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True,  True, False, False,\n        True,  True, False, False, False, False,  True,  True,  True,\n       False,  True,  True, False,  True])\n\n\nFrom this we can see that it selected features :42, 43, 46, 47, 52, 53, 54, 56, 57 and 59. This means that the data set we would train our model on is the following\n\ndata.iloc[:,[42,43,46,47,52,53,54,56,57,59]]\n\n\n\n\n\n  \n    \n      \n      max_rfx\n      min_rfx\n      max_rfz\n      min_rfz\n      max_lfz\n      min_lfz\n      range_rfx\n      range_rfz\n      range_lfx\n      range_lfz\n    \n  \n  \n    \n      0\n      179.4015\n      -232.2776\n      1101.3711\n      48.8063\n      2322.2798\n      -13.4557\n      411.6791\n      1052.5648\n      1006.4781\n      2335.7355\n    \n    \n      1\n      140.1327\n      -189.7241\n      1092.3006\n      51.3111\n      2270.0012\n      -13.8138\n      329.8568\n      1040.9895\n      878.2801\n      2283.8150\n    \n    \n      2\n      106.3177\n      -124.4299\n      1117.9434\n      115.4112\n      1942.1915\n      -9.8942\n      230.7476\n      1002.5322\n      1006.2067\n      1952.0857\n    \n    \n      3\n      138.6102\n      -175.8547\n      1102.4140\n      7.9649\n      2509.2788\n      -9.1957\n      314.4649\n      1094.4491\n      1074.0880\n      2518.4745\n    \n    \n      4\n      175.0215\n      -219.5387\n      1119.0327\n      18.2982\n      2492.2496\n      -9.9647\n      394.5602\n      1100.7345\n      1116.0206\n      2502.2143\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      221.6785\n      -114.1301\n      947.5325\n      31.1219\n      1841.7965\n      -7.0486\n      335.8086\n      916.4106\n      762.2835\n      1848.8451\n    \n    \n      633\n      199.9496\n      -113.4915\n      958.0700\n      26.1562\n      1692.9015\n      -8.2001\n      313.4411\n      931.9138\n      730.4450\n      1701.1016\n    \n    \n      634\n      213.2872\n      -112.7565\n      998.6667\n      44.3632\n      1602.5900\n      -5.3440\n      326.0437\n      954.3035\n      726.2761\n      1607.9340\n    \n    \n      635\n      209.7961\n      -122.3425\n      939.1254\n      29.1908\n      1823.2046\n      -6.8408\n      332.1386\n      909.9346\n      801.7169\n      1830.0454\n    \n    \n      636\n      200.5381\n      -165.5618\n      935.7064\n      19.1782\n      1842.2350\n      -8.3528\n      366.0999\n      916.5282\n      786.5035\n      1850.5878\n    \n  \n\n637 rows × 10 columns\n\n\n\nHowever, 10 was arbitrarily chosen, and may not be the best choice. If we run a series of RFE’s which select for 1 more feature than the last we can then plot these values to find our optimal choice for the number of features.\n\n  \nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nimport matplotlib.pyplot as plt\n\n# define data values\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Linear Regression\")\nplt.show() \n\n\n\n\nFrom this chart we can see that we will be able to use a model with 15 features without losing too much accuracy, so we will next use RFE to find out the variables we want to use.\n\nfeat_select = RFE(model, n_features_to_select = 15, step = 1)\nfeat_select.fit(X,y)\nfeat_select.support_\n\narray([False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False, False, False, False, False,\n       False, False, False, False, False,  True,  True, False, False,\n        True,  True,  True,  True,  True,  True,  True,  True,  True,\n       False,  True,  True,  True,  True])\n\n\nThese refer to the following data set:\n\ndata.iloc[:,[42,43,46,47,48,49,50,51,52,53,54,56,57,58,59]]\n\n\n\n\n\n  \n    \n      \n      max_rfx\n      min_rfx\n      max_rfz\n      min_rfz\n      max_lfx\n      min_lfx\n      max_lfy\n      min_lfy\n      max_lfz\n      min_lfz\n      range_rfx\n      range_rfz\n      range_lfx\n      range_lfy\n      range_lfz\n    \n  \n  \n    \n      0\n      179.4015\n      -232.2776\n      1101.3711\n      48.8063\n      121.2052\n      -885.2729\n      130.9304\n      -414.4391\n      2322.2798\n      -13.4557\n      411.6791\n      1052.5648\n      1006.4781\n      545.3695\n      2335.7355\n    \n    \n      1\n      140.1327\n      -189.7241\n      1092.3006\n      51.3111\n      111.2187\n      -767.0614\n      128.0167\n      -475.8343\n      2270.0012\n      -13.8138\n      329.8568\n      1040.9895\n      878.2801\n      603.8510\n      2283.8150\n    \n    \n      2\n      106.3177\n      -124.4299\n      1117.9434\n      115.4112\n      178.4852\n      -827.7215\n      161.8112\n      -437.5895\n      1942.1915\n      -9.8942\n      230.7476\n      1002.5322\n      1006.2067\n      599.4007\n      1952.0857\n    \n    \n      3\n      138.6102\n      -175.8547\n      1102.4140\n      7.9649\n      170.5486\n      -903.5394\n      187.1682\n      -430.2591\n      2509.2788\n      -9.1957\n      314.4649\n      1094.4491\n      1074.0880\n      617.4273\n      2518.4745\n    \n    \n      4\n      175.0215\n      -219.5387\n      1119.0327\n      18.2982\n      176.3782\n      -939.6424\n      177.3536\n      -420.4205\n      2492.2496\n      -9.9647\n      394.5602\n      1100.7345\n      1116.0206\n      597.7741\n      2502.2143\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      221.6785\n      -114.1301\n      947.5325\n      31.1219\n      69.2794\n      -693.0041\n      121.9773\n      -342.0296\n      1841.7965\n      -7.0486\n      335.8086\n      916.4106\n      762.2835\n      464.0069\n      1848.8451\n    \n    \n      633\n      199.9496\n      -113.4915\n      958.0700\n      26.1562\n      60.6210\n      -669.8240\n      129.1773\n      -342.2472\n      1692.9015\n      -8.2001\n      313.4411\n      931.9138\n      730.4450\n      471.4245\n      1701.1016\n    \n    \n      634\n      213.2872\n      -112.7565\n      998.6667\n      44.3632\n      56.2369\n      -670.0392\n      111.4454\n      -329.7390\n      1602.5900\n      -5.3440\n      326.0437\n      954.3035\n      726.2761\n      441.1844\n      1607.9340\n    \n    \n      635\n      209.7961\n      -122.3425\n      939.1254\n      29.1908\n      67.6610\n      -734.0559\n      149.3230\n      -383.7818\n      1823.2046\n      -6.8408\n      332.1386\n      909.9346\n      801.7169\n      533.1048\n      1830.0454\n    \n    \n      636\n      200.5381\n      -165.5618\n      935.7064\n      19.1782\n      91.0192\n      -695.4843\n      150.5726\n      -252.4603\n      1842.2350\n      -8.3528\n      366.0999\n      916.5282\n      786.5035\n      403.0329\n      1850.5878\n    \n  \n\n637 rows × 15 columns\n\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()\nfeat_select = RFE(model, n_features_to_select = 15, step = 1)\n\nHowever, linear regression is not the only model that can deal with continous data. We can also use random forest regression. If we use random forest regression, and select 15 features again, we get a higher score than we did with linear regression. We also get slightly different features selected.\n\nfeat_select.fit(x_train, y_train)\nprint(feat_select.score(x_test, y_test))\nfeat_select.support_\n\nHowever, we don’t know if 15 is the optimal amount of features. As such, we can follow the same route we did with linear regression and graph our scores.\n\nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Random Forest Regression\")\nplt.show() \n\nThis shows us that any number of features over 10 will give us a model as good as using more. Therefore, to keep things similar between our two models we can use 15 features."
  },
  {
    "objectID": "posts/ML_final/RandomForestRegressorAnalysis.html",
    "href": "posts/ML_final/RandomForestRegressorAnalysis.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "X = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n#Set up\n\n\nfrom RandomForestRegressor import RandomForest\n\n\nrf = RandomForest()\n\n\nrf.fit(x_train, y_train, 1000, 500)\n\nWe also chose to implement and train a second model on our data. We implemented Random Forest Regression, since it is a very powerful model for making predictions with continous data. When we train this model on our whole data set, we get a much better validation score than with linear regression. However, just like with linear regression we can use feature reduction to reduce overfitting.\n\nrf.score(x_test, y_test)\n\n0.5699951488283441\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\nmodel = RandomForestRegressor()\nscores = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores.append(estm.score(x_test,y_test))\n\n\nimport matplotlib.pyplot as plt\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores)  # Plot the chart\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\nplt.title(\"Random Forest Regression\")\nplt.show() \n\n\n\n\nWe can see that this machine doesn’t suffer as much from overfitting, and has a higher validation score than Linear Regression.\n\nx_train = x_train.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\nx_test = x_test.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\n\nrf.fit(x_train, y_train, 1000,500)\nrf.score(x_test, y_test)\n\nUsing the features we found from our RFE we get this score."
  },
  {
    "objectID": "posts/ML_final/index.html",
    "href": "posts/ML_final/index.html",
    "title": "Predicting Bat Speed",
    "section": "",
    "text": "Abstract and Overview of Significance of Topic\nBat speed is a measure of player performance that has become increasingly popular for player development over the past decade. It is typically measured as the speed that the sweet spot of the bat (about 6 inches from the end of the barrel) is traveling when contact is made with the baseball. Bat speed has become increasingly popular due to its high correlation with exit velocity and subsequently hitting metrics such as expected batting average (xBA) or expected weighted on base average (xWOBA). Metrics such as xBA and xWOBA are modern metrics that are used to effectively quantify a players ability to contribute offensively in a positive manner. This increasing popularity in bat speed has led to a related increase in training methodologies based around developing it. Coaches across all levels of play use bat speed as a KPI to validate and inform individualized coaching decisions.\n\n\nFormal Introduction to Topic\nFor our final project, we are using data from the Open Biomechanics Project (OBP) driveline baseball research. Our data captures the biomechanical breakdown of a baseball player’s swing by measuring forces produced by different body parts in three dimensions over exact instances in time, for example, at instance X, player 103’s 2nd swing has a max lead hip force in the y direction of 111.73. The data was captured using a combination of ground force plates, a marker motion capture system, bat speed sensors, and ball flight monitors. The available data is rather robust, accounting for every piece of information that could be responsible for a baseball swing.\nFor our project, our goal is to create a machine learning model that uses this OBP data to identify the most important features of a player’s swing when generating bat speed, and then use those features to accurately predict a player’s bat speed. By comparing an athlete’s true bat speed to their predicted bat speed based on our model, the player could identify how efficiently they are swinging. We hope that this model could be used by baseball players and coaches to address the unique aspects of a player’s swing that could contribute to a higher bat speed, which in turn, would the players reach their fullest potential based on where their inefficiencies lie. Our project can be broken down into two main processes: Identifying the key features that contribute to bat speed. Creating a model that uses the key features to predict bat speed. For the first step, we have decided to run a Recursive Feature Elimination(RFE) on the 60 potential features from our dataset to pull out a smaller number of strong predictive features to use in our model. Next, using those select features, we will run a regression analysis to create a model that can be used to predict a player’s bat speed. Let’s take a closer look at these analyses.\n\n\nValues Statement\nThis project will mainly be utilized by coaches or anybody concerned with player development in baseball. The information that our model would provide to coaches would allow them to make better coaching decisions. In addition to helping coaching staffs, the players themselves would also benefit from more directed training and better evaluation standards. The baseball offseason is very short, so being able to make the most of this time is extremely valuable.\nThis dataset is the first of is the first of its kind to be released as open source. When open-source pitch physics data first became available, it fundamentally changed the way in which baseball was viewed and played. This information allowed both pitchers and hitters to have reliable and measurable feedback on every pitch. The availability of pitch-level biomechanics data has the potential to once again fundamentally change baseball. By working on this project, we are contributing to the larger effort of the baseball community to better understand exactly what makes a baseball swing productive.\n\n\nMaterials and Methods:\n\n\nData\nFrom the OBP dataset, we will be focusing on baseball-hitting data, specifically force plate and joint velocity to predict bat speed. The original datasets can be found here. Driveline baseball research collected this data using a combination of ground force plates, a marker motion capture system, bat speed sensors, and ball flight monitors. Originally, both the force plate and joint velocity datasets had over 1,000,000 observations, with each individual swing including thousands of observations because the swing was broken down by milliseconds. We felt it was unnecessary to keep the time aspect of the dataset, as the velocities produced for each feature variable were very similar from millisecond to millisecond, and the large datasets were difficult to work with. To get rid of the time component and obtain a more reasonably sized data set, we found the minimum, maximum and range of each feature variable in the dataset for every swing. Each swing is labeled by session_swing in our dataset, and each row is a different swing. The session swing is labeled by player ID and swing number, for example, session_swing 111_3 is player 111’s third swing. Not all players have the same number of swings in the dataset, but we don’t think this should have any impact on our results. After eliminating the time aspect, each swing has 60 potential feature variables. The 60 feature variables include the min, max and range of the forces produced by many different body parts in the x, y and z directions during a player’s swing. Some examples include lead shoulder which is the player’s shoulder closest to the pitcher, and rear hip which is the player’s hip furthest from the pitcher.\nOur data possesses some limitations as it exclusively represents male baseball players and doesn’t include any data from female softball players. We think it would be interesting for Driveline baseball research to expand to softball to eliminate some of the gender bias they have inadvertently caused.\n\n\nRecursive Feature Elimination\nRecursive Feature Elimination, or RFE is a recursive algorithm that is used to find the most impactful features out of a large feature set. This is accomplished by training machine learning on all the features, and then removing the least impactful features. This process is repeated with the smaller feature set, until the feature set is of the desired size. This can help prevent overfitting and allow for easier use and training. This does require that the model it is being used to select features has a way to calculate the effect of features, which means that it won’t work for every model, or some determinator has to be created for it to be used. Another drawback is that unless proper testing is done to find out the amount of impactful features, the accuracy can be diminished beyond the benefits of avoiding overfitting.\n\n\nMultiple Linear Regression\nBecause our project is concerned with predicting bat speed, we require a numeric prediction model, rather than a classification prediction model. We decided to use Multiple Linear Regression, which allows us to take two or more features in a dataset to predict a single numeric dependent variable, bat speed. Multiple Linear Regression differs from regular Linear Regression in that you can use more than one feature to predict the target variable. Once built, we can isolate each individual feature and evaluate its impact on the target variable.\nWith our linear regression model, we will be using the Mean Squared Error (MSE) loss function to determine the accuracy and performance of our model.\n\n\nRandom Forest Regression\nRandom Forest Regression is a technique that creates multiple decision trees and averages their outputs to give a final result that often has a high prediction/classification rate. The process involves the user selecting the amount, n, of decision trees to be created, then using the bootstrapping method to randomly select k data samples from the training set. (Bootstrapping is simply the process of randomly selecting subsets from a dataset over a certain number of iterations and a certain number of variables, then the results from each subset are averaged together which returns a more powerful result.) Then, create n decision trees using different random k values. Using the decision trees, predict regression results that can be used on unseen data. Finally, the regression results are all averaged together, returning a final regression output. Random Forests generally provide high accuracy for feature selection as it generates uncorrelated decision trees built by choosing a random set of features for each tree.\n\n\nVariable Overview\nThe features which we have created our data set with fall into two main categories: biomechanics data and force plate data. Beginning with the biomechanics data, we have a set of joints and their associated angular velocities in three planes of motion. We have information on lead hip, lead shoulder, rear hip, rear shoulder, pelvis, torso, and the torso-pelvis complex. For each of these joints, we calculated the range of velocities and maximum velocities for each swing captured. With the force plate data, the lead foot and rear foot are independently observed, and the data is split among the three planes of motion along the x, y, and z axes. For each pairing of foot and plane of motion, we calculated the minimum, maximum, and range of the force produced.\n\n\nThe Process\n\nStep 1: Cleaning the Data\nOur original dataset contained over 1,000,000 observations that were grouped by session_swing. Each swing contained hundreds of observations that analyzed a variety of features over time (~0.0028 seconds between captures). For our project, we wanted to remove this time aspect and instead create a simplified dataset that contained the minimum, maximum, and range values of the features of interest for each swing.\nTo do so, we imported our dataset in R and grouped it by the session_swing variable, and, by using base R functions, calculated the minimum, maximum, and range of each feature variable of interest. We repeated this for the force plate dataset and joint velocity dataset, then used left-join to combine the two datasets to create a conglomerate dataset with all potential predictive features for each session_swing variable.\nWe then added our target vector, max bat speed, from POI_metrics.csv to create our fill dataset that includes our target vector.\nThis process allowed us to get reduce the size of our dataset from over 1,000,000 observations to 665 session_swings.\n\n\nStep 2: RFE Feature Selection\nWe used SKLearn’s RFE feature collection class, which can be found here.\nThe RFE model from the SKLearn class allowed us to gain the base understanding we needed to implement our own version of RFE. After reading through the API and playing around with the RFE feature from SKLearn, we decided to implement our own version of RFE to use with the linear regression model we also implemented. Our RFE function tkaes in three parameters: a feature matrix, X, a target vetor, y, and the number of features we want to be selected, k. The function uses a nested for loop to run through all values i in 1:k and at each iteration, j, checks the weight of all the remaining features that were fit on the linear regression model. From here, the best features are selected as the features with the minimum absolutle value of weight. We use this function in conjunction with our linear regression model to find the number of feautres within our dataset that most accuratley predict the batspeed of a player.\n\n\nStep 3: Building the Models — Multiple Linear Regression and Random Forest\nWe decided to implement our own linear regression model similar to the work we did in class, selecting just the analytic version. We were able to pull out score and predict functions from our previous blog post implementation. We had to modify our fit function by adding a regualarization term to the weight vector to help avoid over/under-fitting the model.\n\n\nStep 4: Testing the Models: Linear Regression and Random Forest\nTo test and train the models, we used an 80/20 train/test split. For both models, we ran a loop to show our the training and testing scores while increasing the number of selected features for the recursive feature elimination model. Once we identified the optimal range of features to use on the Multiple Linear Regression and Random Forest Regression models, we created subsets of our training and testing data to contain the selected features. Finally, we trained and tested our models on the data subsets (80 train/20 split).\n\n\n\nResults and Conclusion\n\nLinear Regression\nBelow shows the effect of increasing the number of features on the accuracy scores produced by the Linear Regression model during RFE. As we can see, our training and testing accuracy scores tend to increase as the number of features increase.\nWe noticed that our model tends to have a higher testing accuracy when the model is ran on fewer features, and has the best training score when the model uses all 60 features. However, because we wanted to identify 10-20 key features, we decided to train and test our model on the 13 best features, which we selected as value that yields the second best testing score.\nThe table shows the 13 most important features as selected by our RFE function. These results were quite unexpected, including values like min_rfx and min_rfy as selected variables of importance. min_rfx and min_rfy represent the minimum rear force produced in the x and y directions, which essentially represent the load, or backwards movement prior to the actual swing itself. Other variables make sense as being some of the most important features, such as range_lead_hip_z, and max_torso_pelvis_x as these are body parts that are essitial in creating the rotational force of a swing to help produce a better bat speed.\nUsing a subset of these 13 features, we trained and tested our model, which produced a testing accuracy score of 38.7%. Unfortunately, this low accuracy indicates that our model isn’t performing as we had hoped. It could mean that our data doesn’t have a strong linear seperability which indicates there is nothing wrong with our model, but rather it isn’t the best model option for our data. Becasue of this, we decided to see if we could produce stronger results by using the Random Forest Regression model.\n\nfrom LinearRegressionAnalytic import LinearRegressionAnalytic\nfrom LinearRegressionAnalytic import rfe\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"biomechanics_dataset_v1.csv\") \n\nnp.random.seed(1)\nX = data.iloc[:,1:60]\ny = data.iloc[:, 60]\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = .2)\n\n\nval_scores = []\ntrain_scores = []\nfor k in range(60):\n    selected_features = rfe(x_train, y_train, k)\n    feature_mask = np.zeros(x_train.shape[1], dtype=bool)\n    #masking to include only the selected features\n    feature_mask[selected_features] = True\n    #subseting x train and test to include only selected feautres\n    X_train_selected = x_train.loc[:, feature_mask]\n    X_test_selected = x_test.loc[:, feature_mask]\n    lr = LinearRegressionAnalytic()\n    #fitting model on selected features\n    lr.fit(X_train_selected, y_train)\n    #appending score to score list\n    val_scores.append(lr.score(X_test_selected, y_test))\n    train_scores.append(lr.score(X_train_selected, y_train))\n\n# plot the results\nimport matplotlib.pyplot as plt\nplt.plot(range(1, 61), val_scores, label='Testing accuracy')\nplt.plot(range(1, 61), train_scores, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.title(\"Accuracy Scores Produced by Linear Regression Model During RFE\")\nplt.show()\n\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\ndisplay(data.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]])\n\n\nx_train_lin = x_train.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\nx_test_lin = x_test.iloc[:,[33, 34, 3, 35, 37, 6, 39, 43, 45, 14, 19, 23, 28]]\n\nlr.fit(x_train_lin, y_train)\nprint(\"Testing Accuracy of Subset:\", lr.score(x_test_lin, y_test))\n\n\n\n\n\n\n\n\n  \n    \n      \n      max_rear_shoulder_x\n      max_rear_shoulder_y\n      range_lead_hip_z\n      max_rear_shoulder_z\n      max_torso_y\n      range_lead_shoulder_z\n      max_torso_pelvis_x\n      min_rfx\n      min_rfy\n      range_rear_shoulder_y\n      range_torso_pelvis_x\n      max_lead_hip_z\n      max_pelvis_y\n    \n  \n  \n    \n      0\n      550.0243\n      514.1198\n      778.1339\n      1188.6807\n      182.7302\n      867.6362\n      182.4743\n      -232.2776\n      -88.0097\n      689.2249\n      233.0842\n      384.3450\n      88.3858\n    \n    \n      1\n      638.6019\n      535.2822\n      960.4793\n      1278.5380\n      196.9712\n      1054.5098\n      236.0902\n      -189.7241\n      -106.2254\n      812.9988\n      306.7874\n      520.8627\n      106.4238\n    \n    \n      2\n      580.0406\n      472.9189\n      784.0413\n      1588.7207\n      248.4432\n      988.9415\n      222.8233\n      -124.4299\n      -84.5785\n      708.1030\n      313.2967\n      433.6955\n      82.5397\n    \n    \n      3\n      635.8561\n      484.2663\n      1036.2757\n      888.1270\n      166.9048\n      1472.7250\n      168.7606\n      -175.8547\n      -122.1629\n      732.5588\n      228.6738\n      489.4716\n      81.4764\n    \n    \n      4\n      566.9714\n      502.2202\n      1093.3019\n      1487.6143\n      191.2448\n      1130.6572\n      220.7400\n      -219.5387\n      -72.5831\n      699.1772\n      286.4758\n      597.7220\n      75.9968\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      631.5529\n      488.3580\n      980.3030\n      1575.2948\n      165.8830\n      1150.6032\n      147.9856\n      -114.1301\n      -173.2356\n      804.6660\n      239.9022\n      354.5130\n      124.7927\n    \n    \n      633\n      571.2316\n      477.7701\n      748.3298\n      1604.9299\n      145.5400\n      1026.0944\n      188.9410\n      -113.4915\n      -157.5923\n      735.1128\n      276.8293\n      324.9995\n      137.1521\n    \n    \n      634\n      549.3600\n      407.3251\n      526.3367\n      1393.4961\n      128.0184\n      1029.3547\n      257.2261\n      -112.7565\n      -111.9854\n      584.3304\n      348.2130\n      207.2101\n      128.8111\n    \n    \n      635\n      623.2650\n      463.8467\n      1248.0062\n      1715.0544\n      136.8013\n      892.8699\n      177.4202\n      -122.3425\n      -161.2802\n      725.1355\n      266.6244\n      282.0038\n      157.1024\n    \n    \n      636\n      599.2501\n      505.9937\n      1433.3273\n      1480.4099\n      143.7898\n      1233.8176\n      169.0549\n      -165.5618\n      -132.0637\n      700.1916\n      234.5590\n      500.9032\n      107.8579\n    \n  \n\n637 rows × 13 columns\n\n\n\nTesting Accuracy of Subset: 0.386607442390802\n\n\n\n\nRandom Forest Regression\nBelow is a graph that displays the training and testing scores produced by the Random Rorest Regression model when ran with RFE over all of the features. As we can see from the graph, this model performed much better than the Linear Regression model. Unlike the Linear Regression model, the Random Rorest model reached a peak around 10 features and maintained consistency at that score, whereas the Linear Regression model had increased variance between scores across all of the features.\nBecause the Random Forest Regression model creates a series of trees using the bootstrapping method, we expected the model to have a better training accuracy and weren’t as concerned with overfitting. We also expected it to have a better accuracy score than the Linear Regression model because it can capture non-linear relationships, which we expect our data to have due to the poor performance of the Multiple Linear Regression model. Aditionally, because of the way the trees are built, the random forest model is less likely to be heavily affected by outliers in the data which will allow it to have a better testing accuracy score.\nWe found that the Random Rorest Regression model had the higher training and validation scores than the Linear Regresion model. The training score reached nearly 100% when ran with more than 10 features. The testing accuracy reached around 65% at 10 features and had a slight increase as the number of features increased. To compare this model with the Linear Regression model, we chose the 15 most important features to test and train the Random Forest model. After training and testing our model on the subset of 15 features, we got a testing accuracy score of 55.4%, which is significantly better than our multiple linear regression model. Considering there is no option for our model to randomly guess since we are predicting a continuous numeric value, we are satisfied with the amount our random forest model learned.\nInterestingly, there is only one feature that the Linear Regression model and the Random Forest Regression model both selected: max_rear_shoulder_y. This feature captures the top hand on the bat, so if the shoulder isn’t moving in the swing, it will hinder the ability to produce enough rotational force from the torso.\nWe were suprised to find that, of the 13 features selected by the Linear Regression model and of the 15 features selected by the Random Forest Regression model, there weren’t more similarities amongst the selected features between the two models. We were hoping to discover a few select features that both models identified as important features, and were interested to find that the majority of the selected features were different.\nWe hypothesize that the dimentionality of the forces produced by each body part may be a factor that contributes toward the challenge of distinguishing significant features. For example, in the Linear Regression model, max_toros_y was selected as an important feautre, and in the Random Forest Regression model, max_torso_z was selected as an important feature. This indicates that the max force produced by the torso is important when creating and predicting bat speed. If we were to run this experiment again, we may try getting rid of the x, y and z components and just use the average of the forces produced by that body part.\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\nmodel = RandomForestRegressor()\nscores_test = []\nscores_train = []\n\nfor x in range(60):\n    estm = RFE(model, n_features_to_select = x+1, step = 1)\n    estm.fit(x_train,y_train)\n    scores_test.append(estm.score(x_test,y_test))\n    scores_train.append(estm.score(x_train,y_train))\n    \nimport matplotlib.pyplot as plt\n\nx_val= [] \n\nfor j in range (60):\n    x_val.append(j+5)\n\n  \nplt.plot(x_val, scores_test, label='Testing accuracy')  # Plot the chart\nplt.plot(x_val, scores_train, label='Training accuracy')\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Accuracy Score\")\nplt.title(\"Random Forest Regression\")\nplt.legend()\nplt.show() \n\n\nfrom RandomForestRegressor import RandomForest\nrf = RandomForest()\n\nx_train_rf = x_train.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\nx_test_rf = x_test.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]]\n\nrf.fit(x_train_rf, y_train, 1000,500)\nprint(rf.score(x_test_rf, y_test))\n\nfeat_select = rfe(x_train, y_train, 14)\nfeat_select\ndisplay(data.iloc[:,[1,8,13,18,21,26,29, 34,38, 46,48,55,56,58]])\n\n0.5541368058167476\n\n\n\n\n\n\n  \n    \n      \n      range_lead_hip_x\n      range_pelvis_y\n      range_rear_shoulder_x\n      range_torso_z\n      range_torso_pelvis_z\n      max_lead_shoulder_z\n      max_pelvis_z\n      max_rear_shoulder_y\n      max_torso_z\n      max_rfz\n      max_lfx\n      range_rfy\n      range_rfz\n      range_lfy\n    \n  \n  \n    \n      0\n      590.6812\n      371.0611\n      838.0101\n      848.3957\n      743.5585\n      617.1386\n      733.6451\n      514.1198\n      775.7749\n      1101.3711\n      121.2052\n      240.6389\n      1052.5648\n      545.3695\n    \n    \n      1\n      536.1970\n      393.4254\n      947.9660\n      814.2556\n      642.8480\n      751.1699\n      799.8748\n      535.2822\n      775.3766\n      1092.3006\n      111.2187\n      297.5680\n      1040.9895\n      603.8510\n    \n    \n      2\n      586.8320\n      396.8130\n      801.1592\n      823.2495\n      853.6754\n      723.6880\n      740.7065\n      472.9189\n      793.0441\n      1117.9434\n      178.4852\n      351.2961\n      1002.5322\n      599.4007\n    \n    \n      3\n      628.4384\n      402.3244\n      958.8471\n      870.6640\n      541.5395\n      810.9479\n      741.3719\n      484.2663\n      819.9890\n      1102.4140\n      170.5486\n      344.0314\n      1094.4491\n      617.4273\n    \n    \n      4\n      595.3172\n      348.1626\n      840.4242\n      809.9368\n      756.6446\n      862.6313\n      770.4950\n      502.2202\n      774.5865\n      1119.0327\n      176.3782\n      262.0008\n      1100.7345\n      597.7741\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      632\n      825.4631\n      350.8260\n      1015.7101\n      919.6701\n      587.4149\n      838.6431\n      679.5463\n      488.3580\n      859.0192\n      947.5325\n      69.2794\n      380.5133\n      916.4106\n      464.0069\n    \n    \n      633\n      768.7166\n      377.7951\n      1020.7830\n      910.0289\n      716.7429\n      799.7780\n      715.3288\n      477.7701\n      846.6447\n      958.0700\n      60.6210\n      365.7219\n      931.9138\n      471.4245\n    \n    \n      634\n      667.8735\n      366.3885\n      978.3792\n      880.3159\n      516.4786\n      815.2906\n      701.0455\n      407.3251\n      820.1794\n      998.6667\n      56.2369\n      331.5945\n      954.3035\n      441.1844\n    \n    \n      635\n      698.0434\n      390.7154\n      1029.8385\n      896.8795\n      616.6955\n      604.2217\n      681.9455\n      463.8467\n      838.6638\n      939.1254\n      67.6610\n      383.2294\n      909.9346\n      533.1048\n    \n    \n      636\n      764.1730\n      348.0633\n      1024.9879\n      907.0956\n      606.3088\n      830.5895\n      677.4534\n      505.9937\n      846.7994\n      935.7064\n      91.0192\n      346.1590\n      916.5282\n      403.0329\n    \n  \n\n637 rows × 14 columns\n\n\n\n\n\nConcluding Discussion\nOverall, our project was sucessful, as we built two models and an RFE function to help us determine the most important features of a swing while predicting bat speed. When we formulated the idea for our project idea, our goal was to deliver python source code that we constructed, along with two jupyter notebooks. One that contained our intial exploration, and one that held our final write up and experiments. We were sucessful in meeting this goal, as we finished with more than two jupyter notebooks, and two python source code files that contained our linear regression with RFE and our random forest model.\nIf we had more time, we would work on finding ways to improve the accuracy of our models. One idea we have to improve accuracy score is to get rid of the dimension factor, as we mentioned above, so each biomechanic force only has one representation instead of three. We hope this would help our models narrow down the important features and produce a better accuracy score. Additionally, we would like to bring in more data to train and test our model on. DriveLine baseball technology is relatively new, so the data is sparse. If the technology was more accesible, we could have more data which would allow our model to improve its success.\n\n\n\nBias\nWe created a model that predicts a player’s bat speed with approximately 65% accuracy, using around 10 features. As with any algorithm, we must consider any assumptions or biased data that could have resulted in systematic error throughout our process. Because our training data came from the Driveline Research and Development, our model was only trained on the players with close proximity or eligibility to their biomechanics facility. According to the U.S. Census, 43.3% of the population of Kent, Washington (home of Driveline R&D) are white individuals, contrasted with 12.2% of the population being black individuals. While the data’s goal is to highlight specific bodily forces and movements that contribute to predicting bat speed rather than demographics like race, age, height, or weight, we must acknowledge that this data is most likely skewed toward white individuals and could be less accurate in predicting the bat speed of players of different races.\nAdditionally, we must highlight that baseball is a male-dominated sport, with the rare exception of a few women playing the sport — see article on Alexis Hopkins. While sports are typically gender segregated for the sake of “fairness” and an acknowledgment that male and female bodies are inherently different and will perform as such, factors like absolute strength and size are not as important in the sport of baseball, as they might be in gender-segregated sports like football and soccer. Rather, the Women’s Sports Foundation explains that baseball involves skills that are combinations of timing, coordination, strength, knowledge of the game, strategies, and control, and argues that bat speed and bat control are more important than absolute strength.\nYet, despite all of this, the Driveline R&D data only contains the biomechanics of male batters. Therefore, if our model were to be improved and implemented, it would only perpetuate the segregation of men and women in this sport. If the data to improve a player’s bat speed can only improve male players, women will continue to be left in the dust.\nDriveline Baseball describes its mission as developing the future of baseball training by finding industry-leading insights; all while publishing those insights back to the baseball community where they belong. However, because baseball is a historically white and male-dominated sport, the “insights” that will be found will only contribute to solidifying that the “baseball community” remains dominated by players that fit those demographics.\nIt is our duty to expand this research and development into more marginalized player communities, such as female athletes and athletes of other races. Then, we can use these insights to create unique training programs that empower and embrace their unique features and help them become the best athletes they can be.\n\n\nApplication\nOur bat speed model could be used by coaching staffs to better inform the decisions that they make. For example, if a given player’s predicted bat speed is higher than their actual recorded bat speed, this would indicate a mechanical inefficiency in their swing. Coaches, with this knowledge, could then direct their focus to finding these mechanical ineffeciencies and correcting them. Players in this group would spend more time on skill acquisition training. On the other hand, if a player has a predicted bat speed which is equal to or lower than their actual bat speed, this would indicate above average efficiency in their swing and their training could be directed more towards general strength and power to increase force production. In either case, our model would help streamline the process from data collection to impact, giving coaches and players the power to have impactful training sessions tailored to each individual.\n\n\nGroup Contribution Statement\nCece Ziegler: Helped with data cleaning. Built RFE function and Linear Regression model. Performed RFE and model experiments. Led writing “Results and Conclusion” sections.\nDavid Byrne: Introduced topic. Managed data cleaning. Led writing of “Abstract and Overview of Significance of Topic”, “Data”, “Variable Overview”, “Values Statement”, and “Application” sections.\nJulia Fairbank: Led writing of “Formal Introduction to Topic”, “Recursive Feature Elimination”, “Multiple Linear Regression”, “Random Forest Regression”, “The Process”, and “Bias” sections.\nSam Ehrsam: Conducted initial RFE experiments with SKLearn library. Built Random Forest Regression model. Performed RFE experiments.\n\n\nPersonal Reflection\nI learned about how to use and manipulate real data to shape it into a useable form. I also learned how to choose and train models on this data, as well as how to work in a team. I am proud of what we were able to accomplish. We were able to implement and train two models on our data, and got one which performed rather well. I intend to carry forward my ability to work and code in a team, as well as work on small portions of a project that will be joined together."
  },
  {
    "objectID": "posts/perceptron/Perceptron_blog_post.html",
    "href": "posts/perceptron/Perceptron_blog_post.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "The code for my update function is as follows: self.w += (2*(int(self.w@X_[var_check].T <0))-1)*X_[var_check]\nself.w is the weight function which is stored as an instance variable of the perceptron. The 2*(int(self.w@X_[var_check].T)) will determine if we are adding or subtracting to w, creating a value of -1 or 1. *X_[var_check].T tells it the magnitude to increase or decrease w by. var_check is our randomly selected variable that we use to check and adjust w.\nIn this experiment, we will use two linearly seperable groups of data points, and run our perceptron on them.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(57698)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nHere we have plotted the points. We can clearly see that they are linearly seperable. We now run our perceptron algorithm on them to find it.\n\nfrom perceptron import Perceptron\np = Perceptron()\np.fit(X, y, 1000)\nprint(p.history)\n\n[0.07, 0.18, 0.02, 0.33, 0.87, 0.48, 0.07, 0.35, 0.04, 0.07, 0.93, 0.45, 0.97, 0.99, 0.94, 0.09, 0.83, 0.01, 0.99, 0.85, 0.03, 0.28, 0.0, 0.02, 0.99, 0.76, 0.05, 0.71, 0.1, 0.01, 0.02, 0.65, 0.82, 0.07, 0.56, 0.05, 0.07, 0.95, 0.06, 0.99, 0.01, 0.98, 0.03, 0.01, 0.82, 0.01, 0.07, 0.88, 0.07, 0.94, 0.9, 0.03, 1.0]\n\n\nAfter running the algorithm, we can print the history variable of the perceptron. This allows us to view the progression of our perceptron, and see how it oscillates between being more and less accurate before arriving at a w value which gives us a linear classifier.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can then graph this linear classifier, and see how it neatly bisects our two groups of data points.\nNext, we will examine two groups of data points which are not linearly seperable.\n\n\nn = 100\np_features = 3\n\nX_1, y_1 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0,0), (0,0)])\n\nfig = plt.scatter(X_1[:,0], X_1[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAfter graphing the points, it is clear that they are not linearly seperable. As such, if we run our perceptron on them we would expect it to not find a linear classifier, and to run until it has reached the maximum allowed runs\n\n\np.fit(X_1, y_1, 100)\nprint(p.history)\n\n[0.49, 0.44, 0.46, 0.46, 0.45, 0.5, 0.59, 0.55, 0.54, 0.56, 0.5, 0.56, 0.56, 0.56, 0.59, 0.56, 0.53, 0.52, 0.5, 0.5, 0.52, 0.47, 0.5, 0.51, 0.51, 0.51, 0.53, 0.47, 0.46, 0.53, 0.49, 0.51, 0.46, 0.51, 0.47, 0.48, 0.47, 0.45, 0.5, 0.51, 0.49, 0.51, 0.48, 0.57, 0.47, 0.44, 0.47, 0.46, 0.53, 0.5, 0.45, 0.51, 0.5, 0.57, 0.5, 0.46, 0.56, 0.49, 0.45, 0.47, 0.56, 0.45, 0.5, 0.46, 0.48, 0.43, 0.5, 0.43, 0.5, 0.48, 0.51, 0.5, 0.53, 0.51, 0.52, 0.6, 0.5, 0.46, 0.52, 0.49, 0.46, 0.47, 0.47, 0.44, 0.55, 0.57, 0.54, 0.51, 0.47, 0.49, 0.54, 0.54, 0.5, 0.42, 0.5, 0.44, 0.44, 0.46, 0.5, 0.52]\n\n\nDoing this, we see what we expected. Our perceptron runs for the maximum allowed time, and never comes up with a w that produces a 1.0 accuracy.\n\nfig = plt.scatter(X_1[:,0], X_1[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nPlotting the line given by w we get a random line, which makes sense.\nWe can now try running the perceptron on data with more than 2 features. We can generate the points, and run our perceptron on them.\n\n\nX_2, y_2 = make_blobs(n_samples = 100, n_features = 5, centers=[(1,1,1,0,0),(0,0,0,0,0)])\n\np.fit(X_2, y_2, 200)\n\nprint(p.history)\n\n[0.68, 0.52, 0.37, 0.53, 0.44, 0.51, 0.55, 0.65, 0.42, 0.41, 0.43, 0.47, 0.48, 0.46, 0.66, 0.49, 0.43, 0.48, 0.72, 0.65, 0.48, 0.55, 0.46, 0.53, 0.74, 0.69, 0.53, 0.44, 0.45, 0.64, 0.72, 0.63, 0.66, 0.51, 0.39, 0.4, 0.46, 0.48, 0.54, 0.47, 0.49, 0.45, 0.36, 0.35, 0.48, 0.42, 0.45, 0.38, 0.55, 0.43, 0.54, 0.51, 0.48, 0.69, 0.67, 0.63, 0.48, 0.46, 0.48, 0.65, 0.59, 0.68, 0.48, 0.65, 0.72, 0.68, 0.69, 0.46, 0.64, 0.65, 0.62, 0.48, 0.43, 0.36, 0.55, 0.46, 0.33, 0.44, 0.5, 0.36, 0.37, 0.54, 0.57, 0.67, 0.67, 0.75, 0.69, 0.69, 0.57, 0.41, 0.48, 0.43, 0.47, 0.56, 0.7, 0.71, 0.69, 0.62, 0.54, 0.54, 0.43, 0.47, 0.33, 0.54, 0.41, 0.38, 0.53, 0.4, 0.49, 0.59, 0.42, 0.46, 0.33, 0.51, 0.57, 0.53, 0.66, 0.65, 0.65, 0.7, 0.68, 0.61, 0.55, 0.54, 0.61, 0.59, 0.74, 0.5, 0.55, 0.49, 0.38, 0.35, 0.57, 0.65, 0.61, 0.43, 0.64, 0.41, 0.36, 0.33, 0.45, 0.54, 0.35, 0.54, 0.36, 0.47, 0.48, 0.36, 0.57, 0.53, 0.42, 0.54, 0.48, 0.33, 0.33, 0.32, 0.29, 0.32, 0.38, 0.35, 0.44, 0.48, 0.37, 0.48, 0.52, 0.34, 0.4, 0.51, 0.39, 0.42, 0.31, 0.3, 0.25, 0.57, 0.39, 0.41, 0.69, 0.7, 0.74, 0.52, 0.47, 0.38, 0.39, 0.48, 0.51, 0.41, 0.62, 0.43, 0.31, 0.41, 0.58, 0.42, 0.41, 0.36, 0.33, 0.39, 0.36, 0.49, 0.57, 0.67]\n\n\nThis set of points most likely is not linearly seperable, as the perceptron did not end up arriving at an accuracy of 1.0. If we test again with slightly altered parameters:\n\nX_2, y_2 = make_blobs(n_samples = 100, n_features = 7, centers=[(2,2,2,2,2,2,2),(-1,-1,-1,-1,-1,-1,-1)])\n\np.fit(X_2, y_2, 200)\nprint(p.history)\n\n[0.99, 0.64, 0.08, 0.78, 0.99, 0.24, 0.85, 0.34, 0.04, 0.35, 1.0]\n\n\nWe can see that the perceptron can still converge on data sets with multiple functions. As such, we can also determine that this set of points are linearly seperable.\nThe runtime of equation 1 is O(p), since the dot product will compute p total multiplications, and it is then added to and multiplied by these same p dimensional arrays, but only a set amount of times, so they don’t add to the complexity."
  },
  {
    "objectID": "posts/Least-Squares/LeastSquaresBlogPost.html",
    "href": "posts/Least-Squares/LeastSquaresBlogPost.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "https://github.com/sehrsam/sehrsam.github.io/blob/main/posts/Least-Squares/Least_Squares_Linear_Regression.py\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(37638)\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nThe first thing that we want to do is to create our set of test datapoints. When p_features = 1, we can visualize them like so.\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train)\n\n\nIf we use the analytic method of fit, we get the following scores:\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.6848\nValidation score = 0.7319\n\n\nSince a score is better the higher that it is, noting that the score cannot exceed 1, the scores of 0.648 and 0.739 are solid. However, the analytic method is not the only way to implement least squares linear regression. We are also able to use gradient descent to set our weights. Using it, we get the following scores:\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8786\nValidation score = 0.9089\n\n\nThese scores are also very good. We can go further, and look at how this score changes over time by taking a look at the score history, and see how it improves over iterations.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nWe can experiment with using more features, seeing how it changes the effectiveness of our models. Since there are going to be more than 2 features, it is tough to visualize our data. However, we can still see the effectiveness of our models by using the score function of our linearSquares class.\n\nn_train = 100\nn_val = 100\np_features = 10\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\nWhen we try our analytic method, we get the following scores:\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train) \n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.6702\nValidation score = 0.6666\n\n\nDoing the same with using gradient descent we get:\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\n\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8851\nValidation score = 0.9168\n\n\nThe scores for these are pretty similar to their p_features = 1 counterparts. If we increase the number of features up to 50 we get the following scores for the analytic method:\n\n\n\n\n\n\n\nn_train = 100\nn_val = 100\np_features = 50\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train) \n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9911\nValidation score = 0.9712\n\n\nAnd for gradient descent we get\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.0001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8479\nValidation score = 0.8598\n\n\nThese are still similar. When we increase it to 99 features we get the following scores for the analytic method\n\nn_train = 100\nn_val = 100\np_features = 99\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train) \n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8992\nValidation score = 0.9416\n\n\n\nAnd the following for gradient descent:\n\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = -5.133801039647608e+122\nValidation score = -6.632516808757505e+122\n\n\nFrom this we can see how overfitting affects the gradient descent method.\nWe can also look at the lasso algorithm from sklearn\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\nDoing the same experiments we just did we can see how it does with many features. First with one feature:\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.0803\nValidation score = 0.2088\n\n\nThen with 10:\n\nn_train = 100\nn_val = 100\np_features = 10\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9405\nValidation score = 0.9056\n\n\nThen with 50:\n\nn_train = 100\nn_val = 100\np_features = 50\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9947\nValidation score = 0.9626\n\n\nAnd finally with 99\n\nn_train = 100\nn_val = 100\np_features = 99\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9988\nValidation score = 0.7424\n\n\nWe can see that the validation score remains high even when the number of features is high, which is different to that of the gradient descent"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Bias/AuditingBias.html",
    "href": "posts/Bias/AuditingBias.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "possible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nI intend to audit for racial bias in employment in my home state of Wisconsin.\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\ndf.head\n\n<bound method NDFrame.head of        AGEP  SCHL  MAR  RELP  DIS  ESP  CIT  MIG  MIL  ANC  NATIVITY  DEAR  \\\n0      12.0   8.0  5.0   2.0  2.0  1.0  1.0  1.0  0.0  2.0       1.0   2.0   \n1      61.0  16.0  1.0   0.0  2.0  0.0  1.0  1.0  4.0  2.0       1.0   2.0   \n2      71.0  18.0  2.0   0.0  2.0  0.0  1.0  1.0  4.0  2.0       1.0   2.0   \n3      82.0  14.0  2.0   0.0  1.0  0.0  1.0  1.0  4.0  1.0       1.0   2.0   \n4      65.0  16.0  5.0  15.0  2.0  0.0  1.0  3.0  2.0  1.0       1.0   2.0   \n...     ...   ...  ...   ...  ...  ...  ...  ...  ...  ...       ...   ...   \n47861  75.0  21.0  5.0   0.0  2.0  0.0  1.0  1.0  3.0  1.0       1.0   2.0   \n47862  27.0  19.0  5.0  13.0  1.0  0.0  1.0  1.0  4.0  1.0       1.0   1.0   \n47863  42.0  13.0  3.0  13.0  2.0  0.0  1.0  1.0  4.0  4.0       1.0   2.0   \n47864  50.0  21.0  5.0   0.0  2.0  0.0  1.0  1.0  4.0  1.0       1.0   2.0   \n47865  80.0   7.0  5.0  16.0  1.0  0.0  1.0  1.0  4.0  4.0       1.0   2.0   \n\n       DEYE  DREM  SEX  group  label  \n0       2.0   2.0  1.0      1  False  \n1       2.0   2.0  1.0      1  False  \n2       2.0   2.0  2.0      1  False  \n3       2.0   2.0  2.0      1  False  \n4       2.0   2.0  1.0      1  False  \n...     ...   ...  ...    ...    ...  \n47861   2.0   2.0  1.0      1  False  \n47862   2.0   2.0  2.0      1   True  \n47863   2.0   2.0  2.0      1   True  \n47864   2.0   2.0  1.0      1   True  \n47865   2.0   1.0  2.0      1  False  \n\n[47866 rows x 17 columns]>\n\n\nThere are 47866 individuals in this data set.\n\ndf.groupby(\"label\").size()\n\nlabel\nFalse    24318\nTrue     23548\ndtype: int64\n\n\nOf these individuals, 24318 are unemployed while 23548 are employed. They are split into the following groups: *White is 1 and Black is 2\n\ndf.groupby(\"group\").size()\n\ngroup\n1    43637\n2     1597\n3      377\n4        4\n5       40\n6      792\n7       18\n8      540\n9      861\ndtype: int64\n\n\nWe are focusing on white and black people in this audit. There are 43637 people who identified as white in this data set and 1597 people who identified as black.\n\ndf.groupby(\"group\")[\"label\"].mean()\n\ngroup\n1    0.502624\n2    0.339386\n3    0.387268\n4    0.250000\n5    0.275000\n6    0.492424\n7    0.333333\n8    0.485185\n9    0.298490\nName: label, dtype: float64\n\n\nIn this data, 50.2% of white people were employed, while 33.9% of black people were employed\nWe can also look at sex, and see how the intersection of these two impcact employment.\n\nrace_and_sex = []\nfor x in range (47866):\n    if df.loc[x,\"group\"]==1 and df.loc[x,\"SEX\"]==1:\n       race_and_sex.append(\"White Male\")\n    elif df.loc[x,\"group\"]==1 and df.loc[x,\"SEX\"]==2:\n        race_and_sex.append(\"White Female\")\n    elif (df.loc[x,\"group\"]==2 and df.loc[x,\"SEX\"]==1):\n        race_and_sex.append(\"Black Male\")\n    elif(df.loc[x,\"group\"]==2 and df.loc[x,\"SEX\"])==2:\n        race_and_sex.append(\"Black Female\")\n    else:\n        race_and_sex.append(\"Other\")\n        \ndf.insert(1, \"RAS\", race_and_sex)        \n\n\nimport seaborn as sns\n\nmeans = df.groupby([\"RAS\"])[\"label\"].mean().reset_index(name = \"mean\")\np = sns.barplot(data = means, x = \"RAS\", y = \"mean\")\n\n\n\n\n\nFrom this, we can see that white males slightly outpace white females, with both being more employed than any other groups. Black females are slightly behind white females, but the big drop is with black males, of whom less than 30% where employed.\n\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\n\n\nmodel = make_pipeline(StandardScaler(), SVC())\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)\n\n0.8434026907328487\n\n\nThe model has an overall accuracy of 84.34%.\n\ng1 =[]\ng2 =[]\nfor i in range (11967):\n    if df.loc[i,\"group\"] == 1:\n        g1.append(i)\n    elif df.loc[i, \"group\"]==2:\n        g2.append(i)\n\nfrom sklearn.metrics import confusion_matrix\n\n\n0.8424336973478939\n\n\n\ny_hat =model.predict(X_test[g1])\ng1Matrix = confusion_matrix (y_hat, y_test[g1])\ng1PPV = g1Matrix[0][0]/(g1Matrix[0][0]+g1Matrix[1][0])\ng1FNR = g1Matrix[1][0]/(g1Matrix[1][0]+g1Matrix[0][0])\ng1FPR = g1Matrix[0][1]/(g1Matrix[1][1]+g1Matrix[0][1])\n(y_hat == y_test[g1]).mean()\ng1Matrix\n\narray([[4495,  651],\n       [1066, 4685]])\n\n\nIt has an accuracy of 84.2% for white individuals and 83.9% for black individuals.\n\ny_hat =model.predict(X_test[g2])\ng2Matrix = confusion_matrix(y_hat, y_test[g2])\ng2PPV = g2Matrix[0][0]/(g2Matrix[0][0]+g2Matrix[1][0])\ng2FNR = g2Matrix[1][0]/(g2Matrix[1][0]+g2Matrix[0][0])\ng2FPR = g2Matrix[0][1]/(g2Matrix[1][1]+g2Matrix[0][1])\n(y_hat == y_test[g2]).mean()\ng2Matrix\n\narray([[174,  23],\n       [ 43, 171]])\n\n\nThe model gives the following PPV, FNR and FPR rates:\n\nprint(\"The PPV for white individuals is : \",g1PPV)\nprint(\"The FNR for white individuals is : \",g1FNR)\nprint(\"The FPR for white individuals is : \",g1FPR)\nprint(\"The PPV for black individuals is : \",g2PPV)\nprint(\"The FNR for black individuals is : \",g2FNR)\nprint(\"The FPR for black individuals is : \",g2FPR)\n\n\nThe PPV for white individuals is :  0.8083078582988671\nThe FNR for white individuals is :  0.19169214170113288\nThe FPR for white individuals is :  0.12200149925037482\nThe PPV for black individuals is :  0.8018433179723502\nThe PPV for black individuals is :  0.19815668202764977\nThe PPV for black individuals is :  0.11855670103092783\n\n\nThis model is calibrated because it doesn’t take into account race, and so given a piece of data, the same score would be returned regardless of which group the individual belonged to. This model also satisfies error rate balance, as the false positive and true negative rates are almost identical across our two groups. This model does not satisfy statistical parity, however, as it predicts that 75% of white people are employed while only predicting 47% of black people are employed.\nThe people who benefit from this algorithm are likely companies that are trying to ascertain interest rates to give on loans or whether or not to give them out at all. While these companies would most likely ask for current employment, they could use this model to predict how likely a person is to remain employed and adjust their decisions accordingly. If this model were to be released and used for this purpose, based on my audit, black individuals would be harmed as they would recieve higher interest rates if they were even granted loans in the first place. Based on my audit, I think that the model displays some bias, which can be seen in the lack of statistical parity. This bias is a a result of a bias in the data itself, which we saw when we looked at the data. This bias worries me, as I fear this model would be used as a way to discriminate while pretending to be fair because it is calibrated and has good error rate balance, but doens’t have the context of redlining and discrimination that go into the features it is trained on. Besides the Bias issues inherent in this model, I don’t have any big objections with deploying this model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "CSCI 0451 Final Project: Create a machine learning model that predicts a players bat speed utilizing Recursive Feature Elimination, Linear Regression and Random Forest Regression models.\n\n\n\n\n\n\nMay 14, 2023\n\n\nCece Ziegler, David Byrne, Julia Fairbank, Sam Ehrsam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my blog"
  }
]