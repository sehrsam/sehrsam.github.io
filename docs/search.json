[
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my blog"
  },
  {
    "objectID": "posts/perceptron/Perceptron_blog_post.html",
    "href": "posts/perceptron/Perceptron_blog_post.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "The code for my update function is as follows: self.w += (2*(int(self.w@X_[var_check].T <0))-1)*X_[var_check]\nself.w is the weight function which is stored as an instance variable of the perceptron. The 2*(int(self.w@X_[var_check].T)) will determine if we are adding or subtracting to w, creating a value of -1 or 1. *X_[var_check].T tells it the magnitude to increase or decrease w by. var_check is our randomly selected variable that we use to check and adjust w.\nIn this experiment, we will use two linearly seperable groups of data points, and run our perceptron on them.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(57698)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nHere we have plotted the points. We can clearly see that they are linearly seperable. We now run our perceptron algorithm on them to find it.\n\nfrom perceptron import Perceptron\np = Perceptron()\np.fit(X, y, 1000)\nprint(p.history)\n\n[0.02, 0.02, 0.48, 0.99, 0.13, 0.56, 0.05, 0.92, 0.03, 0.9, 0.0, 0.63, 0.0, 0.03, 1.0]\n\n\nAfter running the algorithm, we can print the history variable of the perceptron. This allows us to view the progression of our perceptron, and see how it oscillates between being more and less accurate before arriving at a w value which gives us a linear classifier.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nWe can then graph this linear classifier, and see how it neatly bisects our two groups of data points.\nNext, we will examine two groups of data points which are not linearly seperable.\n\n\nn = 100\np_features = 3\n\nX_1, y_1 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0,0), (0,0)])\n\nfig = plt.scatter(X_1[:,0], X_1[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nAfter graphing the points, it is clear that they are not linearly seperable. As such, if we run our perceptron on them we would expect it to not find a linear classifier, and to run until it has reached the maximum allowed runs\n\n\np.fit(X_1, y_1, 100)\nprint(p.history)\n\nDoing this, we see what we expected. Our perceptron runs for the maximum allowed time, and never comes up with a w that produces a 1.0 accuracy.\n\nfig = plt.scatter(X_1[:,0], X_1[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nPlotting the line given by w we get a random line, which makes sense.\nWe can now try running the perceptron on data with more than 2 features. We can generate the points, and run our perceptron on them.\n\n\nX_2, y_2 = make_blobs(n_samples = 100, n_features = 5, centers=[(1,1,1,0,0),(0,0,0,0,0)])\n\np.fit(X_2, y_2, 200)\n\nprint(p.history)\n\n[0.63, 0.61, 0.64, 0.37, 0.49, 0.61, 0.63, 0.7, 0.57, 0.7, 0.57, 0.58, 0.55, 0.54, 0.51, 0.49, 0.54, 0.71, 0.6, 0.46, 0.64, 0.39, 0.35, 0.31, 0.44, 0.22, 0.4, 0.25, 0.31, 0.54, 0.65, 0.49, 0.56, 0.62, 0.63, 0.36, 0.36, 0.42, 0.34, 0.42, 0.39, 0.59, 0.6, 0.73, 0.51, 0.51, 0.46, 0.51, 0.72, 0.72, 0.78, 0.76, 0.78, 0.62, 0.54, 0.71, 0.56, 0.65, 0.58, 0.49, 0.46, 0.4, 0.49, 0.49, 0.37, 0.37, 0.32, 0.46, 0.56, 0.6, 0.66, 0.28, 0.26, 0.57, 0.36, 0.38, 0.35, 0.26, 0.4, 0.56, 0.5, 0.38, 0.31, 0.26, 0.32, 0.49, 0.5, 0.56, 0.58, 0.59, 0.43, 0.59, 0.59, 0.61, 0.68, 0.43, 0.64, 0.35, 0.42, 0.59, 0.71, 0.6, 0.54, 0.38, 0.39, 0.44, 0.52, 0.53, 0.46, 0.53, 0.41, 0.36, 0.33, 0.31, 0.49, 0.28, 0.27, 0.55, 0.6, 0.36, 0.33, 0.33, 0.52, 0.3, 0.27, 0.22, 0.56, 0.51, 0.33, 0.49, 0.52, 0.54, 0.59, 0.59, 0.38, 0.43, 0.26, 0.48, 0.31, 0.42, 0.58, 0.59, 0.48, 0.65, 0.7, 0.63, 0.59, 0.43, 0.44, 0.36, 0.41, 0.41, 0.45, 0.39, 0.4, 0.53, 0.37, 0.31, 0.3, 0.52, 0.46, 0.37, 0.66, 0.54, 0.68, 0.44, 0.42, 0.73, 0.35, 0.38, 0.42, 0.51, 0.59, 0.43, 0.28, 0.27, 0.25, 0.23, 0.53, 0.23, 0.55, 0.63, 0.54, 0.54, 0.63, 0.62, 0.61, 0.57, 0.41, 0.58, 0.36, 0.3, 0.44, 0.47, 0.56, 0.45, 0.34, 0.6, 0.39, 0.34]\n\n\nThis set of points most likely is not linearly seperable, as the perceptron did not end up arriving at an accuracy of 1.0. If we test again with slightly altered parameters:\n\nX_2, y_2 = make_blobs(n_samples = 100, n_features = 7, centers=[(2,2,2,2,2,2,2),(-1,-1,-1,-1,-1,-1,-1)])\n\np.fit(X_2, y_2, 200)\nprint(p.history)\n\nWe can see that the perceptron can still converge on data sets with multiple functions. As such, we can also determine that this set of points are linearly seperable.\nThe runtime of equation 1 is O(p), since the dot product will compute p total multiplications, and it is then added to and multiplied by these same p dimensional arrays, but only a set amount of times, so they don’t add to the complexity."
  }
]