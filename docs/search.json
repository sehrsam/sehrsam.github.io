[
  {
    "objectID": "posts/perceptron/Perceptron_blog_post.html",
    "href": "posts/perceptron/Perceptron_blog_post.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "The code for my update function is as follows: self.w += (2*(int(self.w@X_[var_check].T <0))-1)*X_[var_check]\nself.w is the weight function which is stored as an instance variable of the perceptron. The 2*(int(self.w@X_[var_check].T)) will determine if we are adding or subtracting to w, creating a value of -1 or 1. *X_[var_check].T tells it the magnitude to increase or decrease w by. var_check is our randomly selected variable that we use to check and adjust w.\nIn this experiment, we will use two linearly seperable groups of data points, and run our perceptron on them.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(57698)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nHere we have plotted the points. We can clearly see that they are linearly seperable. We now run our perceptron algorithm on them to find it.\n\nfrom perceptron import Perceptron\np = Perceptron()\np.fit(X, y, 1000)\nprint(p.history)\n\n[0.02, 0.02, 0.48, 0.99, 0.13, 0.56, 0.05, 0.92, 0.03, 0.9, 0.0, 0.63, 0.0, 0.03, 1.0]\n\n\nAfter running the algorithm, we can print the history variable of the perceptron. This allows us to view the progression of our perceptron, and see how it oscillates between being more and less accurate before arriving at a w value which gives us a linear classifier.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nWe can then graph this linear classifier, and see how it neatly bisects our two groups of data points.\nNext, we will examine two groups of data points which are not linearly seperable.\n\n\nn = 100\np_features = 3\n\nX_1, y_1 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(0,0), (0,0)])\n\nfig = plt.scatter(X_1[:,0], X_1[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nAfter graphing the points, it is clear that they are not linearly seperable. As such, if we run our perceptron on them we would expect it to not find a linear classifier, and to run until it has reached the maximum allowed runs\n\n\np.fit(X_1, y_1, 100)\nprint(p.history)\n\nDoing this, we see what we expected. Our perceptron runs for the maximum allowed time, and never comes up with a w that produces a 1.0 accuracy.\n\nfig = plt.scatter(X_1[:,0], X_1[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\nPlotting the line given by w we get a random line, which makes sense.\nWe can now try running the perceptron on data with more than 2 features. We can generate the points, and run our perceptron on them.\n\n\nX_2, y_2 = make_blobs(n_samples = 100, n_features = 5, centers=[(1,1,1,0,0),(0,0,0,0,0)])\n\np.fit(X_2, y_2, 200)\n\nprint(p.history)\n\n[0.63, 0.61, 0.64, 0.37, 0.49, 0.61, 0.63, 0.7, 0.57, 0.7, 0.57, 0.58, 0.55, 0.54, 0.51, 0.49, 0.54, 0.71, 0.6, 0.46, 0.64, 0.39, 0.35, 0.31, 0.44, 0.22, 0.4, 0.25, 0.31, 0.54, 0.65, 0.49, 0.56, 0.62, 0.63, 0.36, 0.36, 0.42, 0.34, 0.42, 0.39, 0.59, 0.6, 0.73, 0.51, 0.51, 0.46, 0.51, 0.72, 0.72, 0.78, 0.76, 0.78, 0.62, 0.54, 0.71, 0.56, 0.65, 0.58, 0.49, 0.46, 0.4, 0.49, 0.49, 0.37, 0.37, 0.32, 0.46, 0.56, 0.6, 0.66, 0.28, 0.26, 0.57, 0.36, 0.38, 0.35, 0.26, 0.4, 0.56, 0.5, 0.38, 0.31, 0.26, 0.32, 0.49, 0.5, 0.56, 0.58, 0.59, 0.43, 0.59, 0.59, 0.61, 0.68, 0.43, 0.64, 0.35, 0.42, 0.59, 0.71, 0.6, 0.54, 0.38, 0.39, 0.44, 0.52, 0.53, 0.46, 0.53, 0.41, 0.36, 0.33, 0.31, 0.49, 0.28, 0.27, 0.55, 0.6, 0.36, 0.33, 0.33, 0.52, 0.3, 0.27, 0.22, 0.56, 0.51, 0.33, 0.49, 0.52, 0.54, 0.59, 0.59, 0.38, 0.43, 0.26, 0.48, 0.31, 0.42, 0.58, 0.59, 0.48, 0.65, 0.7, 0.63, 0.59, 0.43, 0.44, 0.36, 0.41, 0.41, 0.45, 0.39, 0.4, 0.53, 0.37, 0.31, 0.3, 0.52, 0.46, 0.37, 0.66, 0.54, 0.68, 0.44, 0.42, 0.73, 0.35, 0.38, 0.42, 0.51, 0.59, 0.43, 0.28, 0.27, 0.25, 0.23, 0.53, 0.23, 0.55, 0.63, 0.54, 0.54, 0.63, 0.62, 0.61, 0.57, 0.41, 0.58, 0.36, 0.3, 0.44, 0.47, 0.56, 0.45, 0.34, 0.6, 0.39, 0.34]\n\n\nThis set of points most likely is not linearly seperable, as the perceptron did not end up arriving at an accuracy of 1.0. If we test again with slightly altered parameters:\n\nX_2, y_2 = make_blobs(n_samples = 100, n_features = 7, centers=[(2,2,2,2,2,2,2),(-1,-1,-1,-1,-1,-1,-1)])\n\np.fit(X_2, y_2, 200)\nprint(p.history)\n\nWe can see that the perceptron can still converge on data sets with multiple functions. As such, we can also determine that this set of points are linearly seperable.\nThe runtime of equation 1 is O(p), since the dot product will compute p total multiplications, and it is then added to and multiplied by these same p dimensional arrays, but only a set amount of times, so they don’t add to the complexity."
  },
  {
    "objectID": "posts/Least-Squares/LeastSquaresBlogPost.html",
    "href": "posts/Least-Squares/LeastSquaresBlogPost.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import numpy as np\nfrom matplotlib import pyplot as plt\n\nnp.random.seed(37638)\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nThe first thing that we want to do is to create our set of test datapoints. When p_features = 1, we can visualize them like so.\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train)\n\n\nIf we use the analytic method of fit, we get the following scores:\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.6848\nValidation score = 0.7319\n\n\nSince a score is better the higher that it is, noting that the score cannot exceed 1, the scores of 0.648 and 0.739 are solid. However, the analytic method is not the only way to implement least squares linear regression. We are also able to use gradient descent to set our weights. Using it, we get the following scores:\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8786\nValidation score = 0.9089\n\n\nThese scores are also very good. We can go further, and look at how this score changes over time by taking a look at the score history, and see how it improves over iterations.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nWe can experiment with using more features, seeing how it changes the effectiveness of our models. Since there are going to be more than 2 features, it is tough to visualize our data. However, we can still see the effectiveness of our models by using the score function of our linearSquares class.\n\nn_train = 100\nn_val = 100\np_features = 10\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\nWhen we try our analytic method, we get the following scores:\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train) \n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.6702\nValidation score = 0.6666\n\n\nDoing the same with using gradient descent we get:\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\n\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8851\nValidation score = 0.9168\n\n\nThe scores for these are pretty similar to their p_features = 1 counterparts. If we increase the number of features up to 50 we get the following scores for the analytic method:\n\n\n\n\n\n\n\nn_train = 100\nn_val = 100\np_features = 50\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train) \n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9911\nValidation score = 0.9712\n\n\nAnd for gradient descent we get\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.0001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8479\nValidation score = 0.8598\n\n\nThese are still similar. When we increase it to 99 features we get the following scores for the analytic method\n\nn_train = 100\nn_val = 100\np_features = 99\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\nLR = linearSquares()\nLR.fitAnalytic(X_train, y_train) \n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.8992\nValidation score = 0.9416\n\n\n\nAnd the following for gradient descent:\n\n\nLR2 = linearSquares()\nLR2.fitGradient(X_train, y_train, alpha = 0.001, max_iters = 100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = -5.133801039647608e+122\nValidation score = -6.632516808757505e+122\n\n\nFrom this we can see how overfitting affects the gradient descent method.\nWe can also look at the lasso algorithm from sklearn\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\nDoing the same experiments we just did we can see how it does with many features. First with one feature:\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.0803\nValidation score = 0.2088\n\n\nThen with 10:\n\nn_train = 100\nn_val = 100\np_features = 10\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9405\nValidation score = 0.9056\n\n\nThen with 50:\n\nn_train = 100\nn_val = 100\np_features = 50\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9947\nValidation score = 0.9626\n\n\nAnd finally with 99\n\nn_train = 100\nn_val = 100\np_features = 99\nnoise = 0.2\n\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\nL.fit(X_train, y_train)\nprint(f\"Training score = {L.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {L.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.9988\nValidation score = 0.7424\n\n\nWe can see that the validation score remains high even when the number of features is high, which is different to that of the gradient descent"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my blog"
  }
]